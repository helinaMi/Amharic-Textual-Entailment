{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\hp\\Anaconda3\\envs\\new\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\hp\\Anaconda3\\envs\\new\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\hp\\Anaconda3\\envs\\new\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\hp\\Anaconda3\\envs\\new\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\hp\\Anaconda3\\envs\\new\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\hp\\Anaconda3\\envs\\new\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#!/home/selim/DS/python/anaconda3/bin/python\n",
    "from __future__ import print_function\n",
    "\n",
    "from flask import Flask, request, send_from_directory, redirect, render_template, flash, url_for, jsonify, make_response, abort\n",
    "from flask import Flask, render_template\n",
    "from flask_restful import Resource, Api\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys, os\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "%matplotlib inline\n",
    "from markupsafe import Markup\n",
    "from IPython.display import Image, display\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "from gensim import models\n",
    "import base64\n",
    "\n",
    "import matplotlib.font_manager\n",
    "\n",
    "#for tables in Jupyter\n",
    "from IPython.display import HTML, display\n",
    "#import tabulate\n",
    "import urllib.parse\n",
    "#for visualization\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA as sPCA\n",
    "from sklearn import manifold \n",
    "import os\n",
    "from os.path import join, exists\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "#from pandas.lib import TimeValidator\n",
    "import re\n",
    "import string\n",
    "#import urllib2\n",
    "import json\n",
    "from zipfile import ZipFile\n",
    "from gensim.models import KeyedVectors\n",
    "import sys\n",
    "import codecs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import graphviz\n",
    "#from ggplot import *\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.models import model_from_config, model_from_json\n",
    "\n",
    "from keras.layers import Embedding, Dense, Input, Dropout, Reshape, BatchNormalization, TimeDistributed, Lambda, Layer, merge, LSTM, Bidirectional, Convolution1D, GRU, add, concatenate\n",
    "from keras.callbacks import Callback, ModelCheckpoint, TensorBoard, BaseLogger, ReduceLROnPlateau\n",
    "from keras.optimizers import RMSprop, Adam, SGD, Adagrad\n",
    "import sys \n",
    "from keras.utils import CustomObjectScope\n",
    "\n",
    "from codecs import unicode_escape_encode, unicode_escape_decode, raw_unicode_escape_encode,raw_unicode_escape_decode\n",
    "import logging\n",
    "#sys.setrecursionlimit(2960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract D:/ett.json\n"
     ]
    }
   ],
   "source": [
    "dataset_filename = 'multinli_0.9'\n",
    "training_sample_size = 1000 # Change this to -1 if you want all\n",
    "train_filename = ('D:/ett.json')\n",
    "if exists(train_filename):\n",
    "    print('Extract %s' % train_filename)\n",
    "    zipfile = open(train_filename, 'r', encoding = 'utf-8-sig') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>የቤተሰብ ቀን በባሕሩ ዳርቻ ላይ</td>\n",
       "      <td>ምንም እንኳን የቤተሰብ ቀን ቢሆንም ብዙ ሰዎች በባሕሩ ዳርቻ ላይ አሉ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contradiction</td>\n",
       "      <td>ፖሊሱ ሰውየውን መታው</td>\n",
       "      <td>ሴትየዋ የተመታችው በፖሊሱ ነው</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>entailment</td>\n",
       "      <td>ፖሊሱ ሰውየውን መታው</td>\n",
       "      <td>ሰውየው የተመታው በፖሊሱ ነው</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>entailment</td>\n",
       "      <td>አንድ ሰው ስልኩን እያየ ነው</td>\n",
       "      <td>ሰውየው ስልኩን እየተመለከተ ነው</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>አንድ ሰው ስልኩን እየነካካ ነው</td>\n",
       "      <td>ሰውየው ስልኩን እየተመለከተ ነው</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            gold             sentence1  \\\n",
       "0        neutral  የቤተሰብ ቀን በባሕሩ ዳርቻ ላይ   \n",
       "1  contradiction         ፖሊሱ ሰውየውን መታው   \n",
       "2     entailment         ፖሊሱ ሰውየውን መታው   \n",
       "3     entailment    አንድ ሰው ስልኩን እያየ ነው   \n",
       "4        neutral  አንድ ሰው ስልኩን እየነካካ ነው   \n",
       "\n",
       "                                      sentence2  \n",
       "0  ምንም እንኳን የቤተሰብ ቀን ቢሆንም ብዙ ሰዎች በባሕሩ ዳርቻ ላይ አሉ  \n",
       "1                           ሴትየዋ የተመታችው በፖሊሱ ነው  \n",
       "2                            ሰውየው የተመታው በፖሊሱ ነው  \n",
       "3                          ሰውየው ስልኩን እየተመለከተ ነው  \n",
       "4                          ሰውየው ስልኩን እየተመለከተ ነው  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import codecs\n",
    "import io\n",
    "from json import loads\n",
    "\n",
    "def loadDataset(filename, size=-1):\n",
    "    label_category = {\n",
    "        'neutral': 0,\n",
    "        'entailment': 1,\n",
    "        'contradiction': 2\n",
    "    }\n",
    "    dataset = []\n",
    "    sentence1 = []\n",
    "    sentence2 = []\n",
    "    labels = []\n",
    "    with codecs.open(filename, 'r',  'utf-8-sig') as f:\n",
    "        if len(f.readlines(f)) != 0:\n",
    "            f.seek(0)\n",
    "            i = 0\n",
    "         \n",
    "            for line in f:\n",
    "                row = json.loads(str(line).encode())\n",
    "                if size == -1 or i < size:\n",
    "                    dataset.append(row)\n",
    "                    label = row[\"gold\"]\n",
    "                    if label in label_category:\n",
    "                        sentence1.append( row[\"sentence1\"])\n",
    "                        sentence2.append( row[\"sentence2\"])\n",
    "\n",
    "                        labels.append( label_category[label] )\n",
    "                   \n",
    "                \n",
    "        return (dataset, sentence1, sentence2, labels)\n",
    "\n",
    "\n",
    "(train_dataset, train_sentence1, train_sentence2, train_labels) = loadDataset(train_filename, training_sample_size)\n",
    "\n",
    "train_df = pd.DataFrame(train_dataset)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_features = train_df[['sentence1', 'sentence2', 'gold']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FOLDER = os.path.join('static', 'img_pool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v= None\n",
    "sw = None\n",
    "graph =None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathft = 'E:/ftnfl.txt'\n",
    "pathwv = 'E:/wvnfl.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "app.secret_key = b'_5#y2L\"F4Q8z\\n\\xec]/'\n",
    "app.config['UPLOAD_FOLDER'] = IMAGE_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ma():\n",
    "    global w2v,sw\n",
    "    sw = KeyedVectors.load_word2vec_format(pathft)\n",
    "    w2v = KeyedVectors.load_word2vec_format(pathwv)\n",
    "\n",
    "    graph  = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = ma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run model_load.ipynb\n",
    "__name__ == '__main__' and '__file__' not in globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hp\\Anaconda3\\envs\\new\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1242: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\hp\\Anaconda3\\envs\\new\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1344: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ft = ft()\n",
    "wv = wv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    ft\n",
    "    wv\n",
    "    load\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:File `'each_plt.ipynb.py'` not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run each_plt.ipynb\n",
    "__name__ == '__main__' and '__file__' not in globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/disimilarwv')\n",
    "def dis():\n",
    "    return render_template('dissimilar.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/morphemewv')\n",
    "def morp():\n",
    "    return render_template('morp.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/simwv')\n",
    "def simi():\n",
    "    return render_template('similar.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/analogyw')\n",
    "def ana():\n",
    "    return render_template('analogy.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/morphemew')\n",
    "def swsim():\n",
    "    return render_template('swsim.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/wpca')\n",
    "def pll():\n",
    "    return render_template('pca.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/simwv', methods = [\"POST\", \"GET\"])\n",
    "\n",
    "def simwv():\n",
    "    if request.method=='POST':\n",
    "        if 'sentences' not in request.form :\n",
    "            flash('No word post')\n",
    "            redirect(request.url)\n",
    "        elif request.form['sentences'] == '':\n",
    "            flash('No word')\n",
    "            redirect(request.url)\n",
    "        else:\n",
    "            texta = request.form.get('sentences')\n",
    "            try:\n",
    "                results = w2v.most_similar(texta.split())\n",
    "            except:\n",
    "                results = 'Try Again'\n",
    "      \n",
    "            logging.info(\"dissimilar for %s: %s\" % (texta, results))\n",
    "\n",
    "    return render_template('similar.html', sentences=texta,results=results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/simft', methods = [\"POST\", \"GET\"])\n",
    "\n",
    "def simft():\n",
    "    if request.method=='POST':\n",
    "        if 'sentence' not in request.form :\n",
    "            flash('No word post')\n",
    "            redirect(request.url)\n",
    "        elif request.form['sentence'] == '':\n",
    "            flash('No word')\n",
    "            redirect(request.url)\n",
    "        else:\n",
    "            texta = request.form.get('sentence')\n",
    "            try:\n",
    "        \n",
    "\n",
    "                results = sw.most_similar(texta.split())\n",
    "            except:\n",
    "                results='Try Again'\n",
    "      \n",
    "                results.append((results))\n",
    "                results=','.join(str(string) for string in results)\n",
    "                #new_list = codecs.decode(new_list, \"unicode_escape\",errors=\"replace\")\n",
    "                results = '<br>'.join(str(string) for string in results)\n",
    "\n",
    "\n",
    "    return render_template('similar.html', sentence=texta,result=results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/disimilarwv', methods = [\"POST\", \"GET\"])\n",
    "\n",
    "def disimilarwv():\n",
    "    if request.method=='POST':\n",
    "        if 'sentence' not in request.form :\n",
    "            flash('Post at least three words')\n",
    "            redirect(request.url)\n",
    "        elif request.form['sentence'] == '':\n",
    "            flash('No words')\n",
    "            redirect(request.url)\n",
    "        else:\n",
    "            texta = request.form.get('sentence')\n",
    "            try:\n",
    "                resultd = w2v.wv.doesnt_match(texta.split())\n",
    "            except:\n",
    "                resultd = 'Try Again'\n",
    "            logging.info(\"dissimilar for %s: %s\" % (texta, resultd))\n",
    "            y= (\"dissimilar for %s: %s\" % (texta, resultd))\n",
    "          \n",
    "    return render_template('dissimilar.html', y= y, sentence=texta, result=resultd) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/disimilarft', methods = [\"POST\", \"GET\"])\n",
    "\n",
    "def disimilarft():\n",
    "    if request.method=='POST':\n",
    "        if 'sentenced' not in request.form :\n",
    "            flash('Post at least three words')\n",
    "            redirect(request.url)\n",
    "        elif request.form['sentenced'] == '':\n",
    "            flash('No words')\n",
    "            redirect(request.url)\n",
    "        else:\n",
    "            texta = request.form.get('sentenced')\n",
    "            try:\n",
    "                resultd = sw.wv.doesnt_match(texta.split())\n",
    "            except:\n",
    "                resultd = 'Try Again'\n",
    "            logging.info(\"dissimilar for %s: %s\" % (texta, resultd))\n",
    "            x= (\"The most dissimilar for query %s: is  %s\" % (texta, resultd))\n",
    "          \n",
    "    return render_template('dissimilar.html',x=x ,sentenced=texta, resultd=resultd) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/morphemewv', methods = [\"POST\", \"GET\"])\n",
    "\n",
    "def morphemewv():\n",
    "    if request.method=='POST':\n",
    "        if 'sentencew' not in request.form :\n",
    "            flash('No sentence post')\n",
    "            redirect(request.url)\n",
    "        elif request.form['sentencew'] == '':\n",
    "            flash('No sentence')\n",
    "            redirect(request.url)\n",
    "        else:\n",
    "            texta = request.form.get('sentencew')\n",
    "            try:\n",
    "                resultw =  {search_term:[item[0] for item in w2v.wv.most_similar(texta.split(), topn=6)]\n",
    "                        for search_term in texta.split()}\n",
    "            except:\n",
    "                resultw = 'Try Again'\n",
    "      \n",
    "            logging.info(\"MorphemsWV for %s: %s\" % (texta, resultw))\n",
    "       \n",
    "         \n",
    "    return render_template('morp.html', sentencew=texta, result=resultw) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/morphemeft', methods = [\"POST\", \"GET\"])\n",
    "\n",
    "def morphemft():\n",
    "    if request.method=='POST':\n",
    "        if 'sentencef' not in request.form :\n",
    "            flash('No sentence post')\n",
    "            redirect(request.url)\n",
    "        elif request.form['sentencef'] == '':\n",
    "            flash('No sentence')\n",
    "            redirect(request.url)\n",
    "        else:\n",
    "            texta = request.form.get('sentencef')\n",
    "            try:\n",
    "                resultf =  {search_term:[item[0] for item in sw.wv.most_similar(texta.split(), topn=6)]\n",
    "                        for search_term in texta.split()}\n",
    "            except:\n",
    "                resultf = 'Try Again'\n",
    "      \n",
    "            logging.info(\"MorphemsSW for %s: %s\" % (texta, resultf))\n",
    "       \n",
    "         \n",
    "    return render_template('morp.html', sentencef=texta, resultf=resultf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/morphemew', methods = [\"POST\", \"GET\"])\n",
    "\n",
    "def morphemew():\n",
    "    if request.method=='POST':\n",
    "        if 'sentencew' not in request.form :\n",
    "            flash('No sentence post')\n",
    "            redirect(request.url)\n",
    "        elif request.form['sentencew'] == '':\n",
    "            flash('No sentence')\n",
    "            redirect(request.url)\n",
    "        else:\n",
    "            texta = request.form.get('sentencew')\n",
    "            textb = request.form.get('sentencey')\n",
    "            try:\n",
    "                resultt = w2v.similarity(texta,textb)\n",
    "            except:\n",
    "                resultt='Try Again'\n",
    "      \n",
    "\n",
    "            logging.info(\"Similarity score  for %s %s: %s\" % (texta,textb,  resultt))\n",
    "            y= (\"'%s' and '%s' is %s\" %  (texta,textb,  resultt))\n",
    "\n",
    "        \n",
    "\n",
    "         \n",
    "    return render_template('swsim.html',y=y, sentencew=texta,sentencey=textb, result=resultt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/morphemes', methods = [\"POST\", \"GET\"])\n",
    "\n",
    "def morphemes():\n",
    "    if request.method=='POST':\n",
    "        if 'sentences' not in request.form :\n",
    "            flash('No sentence post')\n",
    "            redirect(request.url)\n",
    "        elif request.form['sentences'] == '':\n",
    "            flash('No sentence')\n",
    "            redirect(request.url)\n",
    "        else:\n",
    "            texti = request.form.get('sentences')\n",
    "            textj = request.form.get('sentencei')\n",
    "            try:\n",
    "                result = sw.similarity(texti,textj)\n",
    "            except:\n",
    "                result='Try Again'\n",
    "      \n",
    "\n",
    "            logging.info(\"Similarity score  for %s %s: %s\" % (texti,textj,  result))\n",
    "            x= (\"'%s' and '%s' is %s\" %  (texti,textj,  result))\n",
    "\n",
    "\n",
    "         \n",
    "    return render_template('swsim.html', x=x, sentences=texti,sentencei=textj, res=result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/analogyw', methods = [\"POST\", \"GET\"])\n",
    "\n",
    "def analogyw():\n",
    "    if request.method=='POST':\n",
    "        if 'sentencev' not in request.form :\n",
    "            flash('No sentence post')\n",
    "            redirect(request.url)\n",
    "        elif request.form['sentencev'] == '':\n",
    "            flash('No sentence')\n",
    "            redirect(request.url)\n",
    "        else:\n",
    "            texta = request.form['sentencev']\n",
    "            texta = [texta] if type(texta) == np.str else texta\n",
    "            for example in texta:\n",
    "                    a, b, x = example.split()\n",
    "            try:\n",
    "                \n",
    "        \n",
    "                    predicted = w2v.most_similar([x, b], [a],topn=3)\n",
    "                \n",
    "            except:\n",
    "                predicted='Try Again'\n",
    "            logging.info (\"'%s' is to '%s' as '%s' is to '%s'\" % (a, b, x, predicted))\n",
    "            y = (\"if '%s' is to '%s' as '%s' is to '%s'\" % (a, b, x, predicted))\n",
    "\n",
    "\n",
    "         \n",
    "       \n",
    "         \n",
    "    return render_template('analogy.html',y=y, sentencev=texta, res= predicted) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/analogys', methods = [\"POST\", \"GET\"])\n",
    "\n",
    "def analogys():\n",
    "    if request.method=='POST':\n",
    "        if 'sentences' not in request.form :\n",
    "            flash('No sentence post')\n",
    "            redirect(request.url)\n",
    "        elif request.form['sentences'] == '':\n",
    "            flash('No sentence')\n",
    "            redirect(request.url)\n",
    "        else:\n",
    "            texta = request.form['sentences']\n",
    "            texta = [texta] if type(texta) == np.str else texta\n",
    "            for example in texta:\n",
    "                    a, b, x = example.split()\n",
    "            try:\n",
    "                \n",
    "        \n",
    "                    predicted = sw.most_similar([x, b], [a],topn=3)\n",
    "                \n",
    "            except:\n",
    "                predicted='Try Again'\n",
    "            logging.info (\"'%s' is to '%s' as '%s' is to '%s'\" % (a, b, x, predicted))\n",
    "            x = (\"if '%s' is to '%s' as '%s' is to '%s'\" % (a, b, x, predicted))\n",
    "\n",
    "\n",
    "         \n",
    "       \n",
    "         \n",
    "    return render_template('analogy.html',x =x, sentences=texta, result= predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA for word vectors \n",
    "@app.route('/wpca', methods = [\"POST\", \"GET\"])\n",
    "\n",
    "\n",
    "def show_closest_line():\n",
    "    if request.method=='POST':\n",
    "        if 'sentence' not in request.form :\n",
    "            flash('No sentence post')\n",
    "            redirect(request.url)\n",
    "        elif request.form['sentence'] == '':\n",
    "            flash('No sentence')\n",
    "            redirect(request.url)\n",
    "        else:\n",
    "            font = matplotlib.font_manager.FontProperties(fname = 'C:/Users/hp/Anaconda3/envs/deeg/Library/fonts/abyssinica_sil.ttf').get_name()    \n",
    "            FONT_SIZE = 20\n",
    "            TEXT_KW = dict(fontsize=FONT_SIZE, fontweight='bold', fontproperties=font)\n",
    "\n",
    "            texta = request.form.get('sentence')\n",
    "\n",
    "            n=10\n",
    "            tops = w2v.similar_by_word(texta, topn=n, restrict_vocab=None)\n",
    "\n",
    "            \n",
    "            items = [texta] + [x[0] for x in tops]\n",
    "\n",
    "            wvecs = np.array([w2v.word_vec(wd, use_norm=True) for wd in items])\n",
    "\n",
    "\n",
    "\n",
    "            tsne = manifold.TSNE(n_components=2, init='pca')\n",
    "            coords = tsne.fit_transform(wvecs)\n",
    "            #print(\"kl-divergence: %0.8f\" % tsne.kl_divergence_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            plt.figure(num=None, figsize=(8, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "            plt.tick_params(\n",
    "                axis='both',          \n",
    "                which='both',      \n",
    "                bottom=False,      \n",
    "                left=False,         \n",
    "                labelbottom=False,\n",
    "                labelleft=False)\n",
    "\n",
    "            lim = max([abs(x) for x in coords[:,0] + coords[:,1]])\n",
    "            plt.xlim([-lim,lim])\n",
    "            plt.ylim([-lim,lim])\n",
    "            plt.scatter(coords[2:,0], coords[2:,1])\n",
    "            plt.scatter(coords[0:1,0], coords[0:1,1], color='black')\n",
    "            plt.scatter(coords[1:2,0], coords[1:2,1], color='orange')\n",
    "\n",
    "            for item, x, y in zip(items[2:], coords[2:,0], coords[2:,1]):\n",
    "                plt.annotate( item, xy=(x, y), xytext=(-2, 2), textcoords='offset points', \n",
    "                             ha='right', va='bottom', color='purple', **TEXT_KW)\n",
    "\n",
    "            x0=coords[0,0]\n",
    "            y0=coords[0,1]\n",
    "            plt.annotate( texta , xy=(x0, y0), xytext=(-2, 2), textcoords='offset points', \n",
    "                         ha='right', va='bottom', color='black',**TEXT_KW)\n",
    "\n",
    "            x1=coords[1,0]\n",
    "            y1=coords[1,1] \n",
    "            plt.annotate( items[1] , xy=(x1, y1), xytext=(-2, 2), textcoords='offset points', \n",
    "                         ha='right', va='bottom', color='orange', **TEXT_KW )\n",
    "\n",
    "            ax = plt.gca()\n",
    "\n",
    "            r = math.sqrt( (x1-x0)**2 + (y1-y0)**2 )\n",
    "\n",
    "            circle = plt.Circle((x0, y0), r, color='orange', fill=False)\n",
    "            ax.add_artist(circle)\n",
    "\n",
    "           \n",
    "\n",
    "\n",
    "            img = io.BytesIO() \n",
    "            plt.savefig(img, format='png')\n",
    "            img.seek(0)\n",
    "\n",
    "            plot_url = urllib.parse.quote(base64.b64encode(img.read()).decode())\n",
    "\n",
    "          \n",
    "           # return render_template('pca.html',sentence=texta, plot_url=plot_url)\n",
    "            return render_template('pca.html',sentence=texta, plot_url=plot_url)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA for word vectors \n",
    "@app.route('/fpca', methods = [\"POST\", \"GET\"])\n",
    "\n",
    "\n",
    "def show_closest():\n",
    "    if request.method=='POST':\n",
    "        if 'sentenceb' not in request.form :\n",
    "            flash('No sentence post')\n",
    "            redirect(request.url)\n",
    "        elif request.form['sentenceb'] == '':\n",
    "            flash('No sentence')\n",
    "            redirect(request.url)\n",
    "        else:\n",
    "            font = matplotlib.font_manager.FontProperties(fname = 'C:/Users/hp/Anaconda3/envs/deeg/Library/fonts/abyssinica_sil.ttf').get_name()    \n",
    "            FONT_SIZE = 20\n",
    "            TEXT_KW = dict(fontsize=FONT_SIZE, fontweight='bold', fontproperties=font)\n",
    "\n",
    "            texta = request.form.get('sentenceb')\n",
    "\n",
    "            n=10\n",
    "            tops = sw.similar_by_word(texta, topn=n, restrict_vocab=None)\n",
    "\n",
    "            \n",
    "            items = [texta] + [x[0] for x in tops]\n",
    "\n",
    "            wvecs = np.array([w2v.word_vec(wd, use_norm=True) for wd in items])\n",
    "\n",
    "\n",
    "\n",
    "            tsne = manifold.TSNE(n_components=2, init='pca')\n",
    "            coords = tsne.fit_transform(wvecs)\n",
    "            #print(\"kl-divergence: %0.8f\" % tsne.kl_divergence_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            plt.figure(num=None, figsize=(8, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "            plt.tick_params(\n",
    "                axis='both',          \n",
    "                which='both',      \n",
    "                bottom=False,      \n",
    "                left=False,         \n",
    "                labelbottom=False,\n",
    "                labelleft=False)\n",
    "\n",
    "            lim = max([abs(x) for x in coords[:,0] + coords[:,1]])\n",
    "            plt.xlim([-lim,lim])\n",
    "            plt.ylim([-lim,lim])\n",
    "            plt.scatter(coords[2:,0], coords[2:,1])\n",
    "            plt.scatter(coords[0:1,0], coords[0:1,1], color='black')\n",
    "            plt.scatter(coords[1:2,0], coords[1:2,1], color='orange')\n",
    "\n",
    "            for item, x, y in zip(items[2:], coords[2:,0], coords[2:,1]):\n",
    "                plt.annotate( item, xy=(x, y), xytext=(-2, 2), textcoords='offset points', \n",
    "                             ha='right', va='bottom', color='purple', **TEXT_KW)\n",
    "\n",
    "            x0=coords[0,0]\n",
    "            y0=coords[0,1]\n",
    "            plt.annotate( texta , xy=(x0, y0), xytext=(-2, 2), textcoords='offset points', \n",
    "                         ha='right', va='bottom', color='black',**TEXT_KW)\n",
    "\n",
    "            x1=coords[1,0]\n",
    "            y1=coords[1,1] \n",
    "            plt.annotate( items[1] , xy=(x1, y1), xytext=(-2, 2), textcoords='offset points', \n",
    "                         ha='right', va='bottom', color='orange', **TEXT_KW )\n",
    "\n",
    "            ax = plt.gca()\n",
    "\n",
    "            r = math.sqrt( (x1-x0)**2 + (y1-y0)**2 )\n",
    "\n",
    "            circle = plt.Circle((x0, y0), r, color='orange', fill=False)\n",
    "            ax.add_artist(circle)\n",
    "\n",
    "           \n",
    "\n",
    "\n",
    "            img = io.BytesIO() \n",
    "            plt.savefig(img, format='png')\n",
    "            img.seek(0)\n",
    "\n",
    "            plot_url = urllib.parse.quote(base64.b64encode(img.read()).decode())\n",
    "\n",
    "          \n",
    "            return render_template('pca.html',sentenceb=texta, plot_url=plot_url)\n",
    "    #return render_template('pca.html',sentenceb=texta, plot=plot_url)\n",
    "\n",
    "                \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/', methods = [\"POST\", \"GET\"])\n",
    "def index():\n",
    "   \n",
    "\n",
    "   \n",
    "        \n",
    "    return render_template('indexx.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/layout', methods = [\"POST\", \"GET\"])\n",
    "def home():\n",
    "    return render_template('layout.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/grid', methods =[\"POST\", \"GET\"])\n",
    "def grid():\n",
    "\n",
    "   \n",
    "\n",
    "    pltal = plotallv\n",
    "    accuracyw = historyv['acc'][4]\n",
    "\n",
    "    return render_template('grid.html',pltal=pltal,accuracyw=accuracyw )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/grida', methods =[\"POST\", \"GET\"])\n",
    "\n",
    "def grida():\n",
    "\n",
    "    pltalf=plotallf\n",
    "    accuracyf = historyf['acc'][4]\n",
    "\n",
    "\n",
    "    return render_template('grida.html', pltalf=pltalf,accuracyf=accuracyf)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/plot')\n",
    "def plot():\n",
    "    img_filename = os.path.join(app.config['UPLOAD_FOLDER'], 'training-history-comparison.png')\n",
    "    return render_template('plot.html', img_filename=img_filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datapoint(texta, textb):\n",
    "    texta = [texta] if type(texta) == np.str else texta\n",
    "    textb = [textb] if type(textb) == np.str else textb\n",
    "\n",
    "    tokenizer = configf['tokenizer']['tokenizer']\n",
    "    texta = tokenizer.texts_to_sequences(texta)\n",
    "    textb = tokenizer.texts_to_sequences(textb)\n",
    "\n",
    "    texta = pad_sequences(texta, maxlen = configf['tokenizer']['max_seq_length'])\n",
    "    textb = pad_sequences(textb, maxlen = configf['tokenizer']['max_seq_length'])\n",
    "    return(texta,textb)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_entft(texta, textb):\n",
    "    ret = dict()\n",
    "    texta, textb = prepare_datapoint(texta,textb)\n",
    "    with graphf.as_default():\n",
    "            ent_class = modelf.predict([texta,textb], batch_size=1)[0]\n",
    "    argmax_ent = np.argmax(ent_class)\n",
    "    entel_class_txt  = ([\"Neutral\", \"Postive\", \"Negative\"][np.argmax(ent_class)]+\" entailment\" )\n",
    "    ent_score = ent_class[argmax_ent]\n",
    "    return(entel_class_txt, ent_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_entwv(texta, textb):\n",
    "    ret = dict()\n",
    "    texta, textb = prepare_datapoint(texta,textb)\n",
    "    with graphv.as_default():\n",
    "            ent_class = modelv.predict([texta,textb], batch_size=1)[0]\n",
    "    argmax_ent = np.argmax(ent_class)\n",
    "    entel_class_txt  = ([\"Neutral\", \"Postive\", \"Negative\"][np.argmax(ent_class)]+\" entailment\" )\n",
    "    ent_score = ent_class[argmax_ent]\n",
    "    return(entel_class_txt, ent_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_entwv(texta, textb):\n",
    "    ret = dict()\n",
    "    texta, textb = prepare_datapoint(texta,textb)\n",
    "    with graphv.as_default():\n",
    "            ent_class = modelv.predict([texta,textb], batch_size=1)[0]\n",
    "    argmax_ent = np.argmax(ent_class)\n",
    "    entel_class_txt  = ([\"Neutral\", \"Postive\", \"Negative\"][np.argmax(ent_class)]+\" entailment\" )\n",
    "    ent_score = ent_class[argmax_ent]\n",
    "    return(entel_class_txt, ent_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/ft', methods = [\"POST\", \"GET\"])\n",
    "\n",
    "def ft():\n",
    "    sone=[]\n",
    "    sonemis=[]\n",
    "\n",
    "    new_list =[]\n",
    "    new_listmis =[]\n",
    "\n",
    "    counts=0\n",
    "    countf=0\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    for (i) in range(30):\n",
    "        \n",
    "        trains1, trains2, label = train_df_features.iloc[i]\n",
    "        sentence = trains1\n",
    "        sentencea = trains2\n",
    "       \n",
    "\n",
    "        trains1, trains2 =prepare_datapoint(trains1,trains2)\n",
    "      \n",
    "        \n",
    "        with graphf.as_default(): \n",
    "            pridiction = modelf.predict([trains1, trains2])[0]\n",
    "        x =np.argmax(pridiction)\n",
    "        r= ([\"neutral\", \"entailment\", \"contradiction\"][np.argmax(pridiction)])\n",
    "   \n",
    "       \n",
    "      \n",
    "     \n",
    "        \n",
    "        if label==r:\n",
    "            sone.append('sentence1: ' +  (sentence))\n",
    "            sone.append('sentence2:  '+  (sentencea))\n",
    "            sone.append('actual_label: '+(label))\n",
    "            sone.append('pridicted_label: '+ r)\n",
    "            counts+=1\n",
    "        if label!=r:\n",
    "            sonemis.append('sentence1: ' +  (sentence))\n",
    "            sonemis.append('sentence2:  '+  (sentencea))\n",
    "            sonemis.append('actual_label: '+(label))\n",
    "            sonemis.append('pridicted_label: '+ r)\n",
    "            countf+=1\n",
    "                \n",
    "        #new_list.append= [str(sone).split(maxsplit=0)]\n",
    "        #print(len(sone))\n",
    "\n",
    "        #new_list=[re.sub(',/n ', ' ', s.strip()) for s in sone]\n",
    "        #new_list='[' + ' '(i for item in sone)+ ']'\n",
    "        #new_list.append([string.replace((\"\"\",\"\"\"), \"\"\" \"\"\") for string in sone])\n",
    "        new_list=','.join(str(string) for string in sone)\n",
    "        #new_list = codecs.decode(new_list, \"unicode_escape\",errors=\"replace\")\n",
    "        new_list = '<br>'.join(str(string) for string in sone)\n",
    "        new_listmis=','.join(str(string) for string in sonemis)\n",
    "        #new_list = codecs.decode(new_list, \"unicode_escape\",errors=\"replace\")\n",
    "        new_listmis = '<br>'.join(str(string) for string in sonemis)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        #t =(train_df_features.iloc[i][0])\n",
    "        #train_df.append(pd.DataFrame.iloc[i])\n",
    "        #sum=0\n",
    "        #e = sum([i for z in label[i] if z==r[i]])\n",
    "\n",
    "            \n",
    "            \n",
    "       \n",
    "            \n",
    "        \n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "    #print(new_list)\n",
    "    #print(new_listmis)\n",
    "\n",
    "    #print(sone)\n",
    "    #print(sonemis)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    if request.method=='POST':\n",
    "        if not request.form[\"sentencea\"]:\n",
    "            flash('No Premises post')\n",
    "            redirect(request.url)\n",
    "        elif not request.form[\"sentenceb\"]:\n",
    "            flash('No Hypotesis')\n",
    "            redirect(request.url)\n",
    "        elif request.form['sentencea'] == '' and  request.form['sentenceb'] == '':\n",
    "            flash('Please Enter the two Sentences')\n",
    "            redirect(request.url)\n",
    "\n",
    "        else:\n",
    "\n",
    "            textc = request.form['sentencea']\n",
    "            textd = request.form['sentenceb']\n",
    "            pred, score = predict_entft(textc, textd)\n",
    "   \n",
    "       \n",
    "\n",
    "            return render_template(\"entailment_classification_result.html\", sentencea=textc, pred= pred, score= score, sentenceb=textd)\n",
    "    return render_template('ec.html' ,d=new_list, m= new_listmis ,e=counts, n=countf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/wv', methods = [\"POST\", \"GET\"])\n",
    "\n",
    "def wv():\n",
    "    sone=[]\n",
    "    sonemis=[]\n",
    "\n",
    "    new_list =[]\n",
    "    new_listmis =[]\n",
    "\n",
    "    counts=0\n",
    "    countf=0\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    for (i) in range(30):\n",
    "        \n",
    "        trains1, trains2, label = train_df_features.iloc[i]\n",
    "        sentence = trains1\n",
    "        sentencea = trains2\n",
    "       \n",
    "\n",
    "        trains1, trains2 =prepare_datapoint(trains1,trains2)\n",
    "      \n",
    "        \n",
    "        with graphv.as_default(): \n",
    "            pridiction = modelv.predict([trains1, trains2])[0]\n",
    "        x =np.argmax(pridiction)\n",
    "        r= ([\"neutral\", \"entailment\", \"contradiction\"][np.argmax(pridiction)])\n",
    "   \n",
    "       \n",
    "      \n",
    "     \n",
    "        \n",
    "        if label==r:\n",
    "            sone.append('sentence1: ' +  (sentence))\n",
    "            sone.append('sentence2:  '+  (sentencea))\n",
    "            sone.append('actual_label: '+(label))\n",
    "            sone.append('pridicted_label: '+ r)\n",
    "            counts+=1\n",
    "        if label!=r:\n",
    "            sonemis.append('sentence1: ' +  (sentence))\n",
    "            sonemis.append('sentence2:  '+  (sentencea))\n",
    "            sonemis.append('actual_label: '+(label))\n",
    "            sonemis.append('pridicted_label: '+ r)\n",
    "            countf+=1\n",
    "                \n",
    "        #new_list.append= [str(sone).split(maxsplit=0)]\n",
    "        #print(len(sone))\n",
    "\n",
    "        #new_list=[re.sub(',/n ', ' ', s.strip()) for s in sone]\n",
    "        #new_list='[' + ' '(i for item in sone)+ ']'\n",
    "        #new_list.append([string.replace((\"\"\",\"\"\"), \"\"\" \"\"\") for string in sone])\n",
    "        new_list=','.join(str(string) for string in sone)\n",
    "        #new_list = codecs.decode(new_list, \"unicode_escape\",errors=\"replace\")\n",
    "        new_list = '<br>'.join(str(string) for string in sone)\n",
    "        new_listmis=','.join(str(string) for string in sonemis)\n",
    "        #new_list = codecs.decode(new_list, \"unicode_escape\",errors=\"replace\")\n",
    "        new_listmis = '<br>'.join(str(string) for string in sonemis)\n",
    "   \n",
    "    if request.method=='POST':\n",
    "        if 'sentencea' not in request.form or 'sentenceb' not in request.form:\n",
    "            flash('No sentence post')\n",
    "            redirect(request.url)\n",
    "        elif request.form['sentencea'] == '' or  request.form['sentenceb'] == '':\n",
    "            flash('No sentence')\n",
    "            redirect(request.url)\n",
    "        else:\n",
    "\n",
    "            textc = request.form['sentencea']\n",
    "            textd = request.form['sentenceb']\n",
    "            pred, score = predict_entwv(textc, textd)\n",
    "           \n",
    "            return render_template(\"entailment_classificationv_result.html\", sentencea=textc, pred= pred, score= score, sentenceb=textd)\n",
    "    return render_template('entailment_classificationv.html',d=new_list, m= new_listmis, e=counts, n=countf ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:44:34] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:44:34] \"\u001b[37mGET /static/vendor/animate.css/animate.min.css HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:44:34] \"\u001b[37mGET /static/vendor/owl.carousel/assets/owl.carousel.min.css HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:44:34] \"\u001b[37mGET /static/vendor/venobox/venobox.css HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:44:34] \"\u001b[37mGET /static/vendor/bootstrap/css/bootstrap.min.css HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:44:34] \"\u001b[37mGET /static/vendor/boxicons/css/boxicons.min.css HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:44:34] \"\u001b[37mGET /static/vendor/icofont/icofont.min.css HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:44:34] \"\u001b[37mGET /static/css/stylee.css HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:44:34] \"\u001b[37mGET /static/img/com.png HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:44:34] \"\u001b[37mGET /static/vendor/jquery/jquery.min.js HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:44:34] \"\u001b[37mGET /static/img/copare.png HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:44:34] \"\u001b[37mGET /static/vendor/php-email-form/validate.js HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:44:34] \"\u001b[37mGET /static/vendor/jquery.easing/jquery.easing.min.js HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:44:34] \"\u001b[37mGET /static/vendor/isotope-layout/isotope.pkgd.min.js HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:44:34] \"\u001b[37mGET /static/vendor/bootstrap/js/bootstrap.bundle.min.js HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:44:34] \"\u001b[37mGET /static/js/main.js HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:44:34] \"\u001b[37mGET /static/vendor/venobox/venobox.min.js HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:44:34] \"\u001b[37mGET /static/vendor/owl.carousel/owl.carousel.min.js HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:44:34] \"\u001b[37mGET /static/img/slide/LStm.png HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:44:34] \"\u001b[37mGET /static/img/slide/thim.jpg HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:44:34] \"\u001b[37mGET /static/img/slide/cover.jpg HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:44:34] \"\u001b[37mGET /static/vendor/icofont/fonts/icofont.woff2 HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:44:34] \"\u001b[37mGET /static/vendor/boxicons/fonts/boxicons.woff2 HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:44:39] \"\u001b[37mGET /wv HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:44:39] \"\u001b[33mGET /static/vendor/owl.carousel/static/owl.carousel.min.css HTTP/1.1\u001b[0m\" 404 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:44:39] \"\u001b[37mGET /static/img/trrssm.png HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:44:39] \"\u001b[37mGET /static/img/valrssm.png HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:45:39] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:45:44] \"\u001b[37mGET /morphemewv HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:45:44] \"\u001b[33mGET /static/vendor/owl.carousel/static/owl.carousel.min.css HTTP/1.1\u001b[0m\" 404 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:45:47] \"\u001b[37mGET /analogyw HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:45:47] \"\u001b[33mGET /static/vendor/owl.carousel/static/owl.carousel.min.css HTTP/1.1\u001b[0m\" 404 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:45:50] \"\u001b[37mPOST /analogys HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Nov/2020 16:45:50] \"\u001b[33mGET /static/vendor/owl.carousel/static/owl.carousel.min.css HTTP/1.1\u001b[0m\" 404 -\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "\n",
    "    app.run()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
